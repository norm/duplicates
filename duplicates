#!/usr/bin/env perl

use Modern::Perl    '2012';
use Cwd             qw( abs_path );
use Digest::SHA1;
use File::Next;
use File::stat;
use FileHandle;
use Getopt::Std;
use Number::Bytes::Human    qw( format_bytes );
use Pod::Usage;
use Redis;
use Try::Tiny;

use constant NAMESPACE  => 'duplicates';
use constant SET_KEY    => sprintf '%s:%s', NAMESPACE, 'all';
use constant TOP_WASTED => 10;

$| = 1;
my $redis = Redis->new();

my $arg = shift;

my %option;
getopts('hv', \%option);
pod2usage() if $option{'h'} or !defined $arg or $arg eq '-h';

scan_directory( @ARGV )
    if 'scan' eq $arg;
list_duplicates_in_directory( @ARGV )
    if 'list' eq $arg;
list_all_duplicates( @ARGV )
    if 'list-all' eq $arg;
list_matches( @ARGV )
    if 'matches' eq $arg;
delete_duplicates( @ARGV )
    if 'delete' eq $arg;
rescan_duplicates()
    if 'rescan' eq $arg;
rescan_duplicates_within_directory( @ARGV )
    if 'rescan-dir' eq $arg;
wipe_data()
    if 'wipe' eq $arg;

if ( 'stats' eq $arg ) {
    output_stats();
}
else {
    die "Unknown arg: $arg\n";
}
exit;



sub scan_directory {
    my @directories = @_;

    # default to current directory
    push @directories, '.'
        if $#directories < 0;

    foreach my $directory ( @directories ) {
        $directory = abs_path( $directory );
        say $directory;

        my $count = 0;
        my $files = File::Next::files( $directory );
        while( my $file = $files->() ) {
            $count++;
            if ( $option{'v'} ) {
                say "  [$count] $file";
            }
            else {
                my $status = sprintf '  [%d] %s', $count, $file;
                printf "%s\r", substr( $status, 0, 79 );
            }

            try {
                my $digest = get_sha_of_file( $file );
                register_file( $file, $digest );
            }
            catch {
                warn "$file: $!";
            };

        }
        say '';
    }
}
sub list_duplicates_in_directory {
    my @directories = @_;

    # default to current directory
    push @directories, '.'
        if $#directories < 0;

    foreach my $directory ( @directories ) {
        $directory = abs_path( $directory );

        my @duplicates = $redis->zrevrangebyscore( SET_KEY, '+inf', 2 );
        my $count = 0;

        foreach my $dupe ( @duplicates ) {
            $dupe =~ m{:(.*)};

            my @keys = $redis->hkeys( $dupe );
            foreach my $key ( @keys ) {
                next unless $key =~ m{^$directory};
                say $key;
            }
        }
    }
}
sub list_all_duplicates {
    my @directories = $@;

    my @duplicates = $redis->zrevrangebyscore( SET_KEY, '+inf', 2 );
    foreach my $dupe ( @duplicates ) {
        $dupe =~ m{:(.*)};

        say "${1}:"
            if $option{'v'};
        foreach my $key ( $redis->hkeys( $dupe ) ) {
            say( ( $option{'v'} ? '    ' : '' ) . $key );
        }
        say ''
            if $option{'v'};
    }
}
sub list_matches {
    my @list = @_;

    foreach my $item ( @list ) {
        say "${item}:"
            if $option{'v'};

        my @matches = get_matching_files( $item );
        foreach my $match ( @matches ) {
            next unless $match ne $item;
            say( ( $option{'v'} ? '    ' : '' ) . $match );
        }

        say ''
            if $option{'v'};
    }
}
sub delete_duplicates {
    my @list = @_;

    my $rescan_warning = 0;

    foreach my $directory ( @list ) {
        my $files = File::Next::files( $directory );
        while( my $file = $files->() ) {
            my $digest  = get_sha_of_file( $file );
            my @matches = get_matching_files( $file, $digest );
            
            # only delete it if other duplicates exist
            next unless $#matches;

            # double check those duplicates still exist
            foreach my $match ( @matches ) {
                next if $match eq $file;
                
                my $match_digest = get_sha_of_file( $match );
                if ( !defined $match_digest or $match_digest ne $digest ) {
                    $rescan_warning = $match;
                    next;
                }

                unlink $file
                    or die "Couldn't delete $file: $!";

                deregister_file( $file, $digest );
                say "Deleted $file"
                    if $option{'v'};
                last;
            }
        }
        chdir $directory;
        system 'find', '.', '-type', 'd', '-delete', '-empty';
    }

    if ( $rescan_warning ) {
        warn "$rescan_warning no longer a duplicate\n";
        warn "Run 'duplicates rescan' and try again\n";
    }
}
sub rescan_duplicates {
    my @duplicates = $redis->zrevrangebyscore( SET_KEY, '+inf', 1 );

    foreach my $duplicate ( @duplicates ) {
        $duplicate =~ m{:(.*)};
        my $orig_digest = $1;

        foreach my $file ( $redis->hkeys( $duplicate ) ) {
            my $digest = get_sha_of_file( $file ) // '';
            if ( $orig_digest ne $digest ) {
                say "Updating $file"
                    if $option{'v'};

                deregister_file( $file, $orig_digest );
                register_file( $file, $digest )
                    if $digest;
            }
        }
    }
}
sub rescan_duplicates_within_directory {
    my @directories = @_;

    # default to current directory
    push @directories, '.'
        if $#directories < 0;

    my @duplicates = $redis->zrevrangebyscore( SET_KEY, '+inf', 1 );

    foreach my $duplicate ( @duplicates ) {
        $duplicate =~ m{:(.*)};
        my $orig_digest = $1;

        foreach my $file ( $redis->hkeys( $duplicate ) ) {
            my $within = 0;
            foreach my $directory ( @directories ) {
                $within = 1
                    if $file =~ m{^$directory};
            }
            next unless $within;

            my $digest = get_sha_of_file( $file ) // '';
            if ( $orig_digest ne $digest ) {
                say "Updating $file"
                    if $option{'v'};

                deregister_file( $file, $orig_digest );
                register_file( $file, $digest )
                    if $digest;
            }
        }
    }
}
sub output_stats {
    my @directories = $@;

    my @duplicates = $redis->zrevrangebyscore( SET_KEY, '+inf', 2 );

    my $count  = 0;
    my $wasted = 0;

    my $smallest = 0;
    my %top_files;

    foreach my $dupe ( @duplicates ) {
        my %keys = $redis->hgetall( $dupe );

        my( undef, $size ) = each %keys;
        my $dupes = scalar( keys %keys ) - 1;

        $wasted += ( $size * $dupes );
        $count += $dupes;

        if ( $option{'v'} ) {
            $top_files{$size} = []
                if ! defined $top_files{$size};
            push @{$top_files{$size}}, keys %keys;

            if ( $size > $smallest and scalar keys %top_files > TOP_WASTED ) {
                my @sizes = sort { $a <=> $b } keys %top_files;

                delete $top_files{$sizes[0]};
                my $smallest = $sizes[1];
            }
        }
    }

    say sprintf "%d duplicated files, %s wasted space",
        $count, format_bytes($wasted)
            if $count;

    if ( $option{'v'} ) {
        say '';
        for my $size ( sort { $b <=> $a } keys %top_files ) {
            say format_bytes($size);
            foreach my $file ( @{ $top_files{$size} } ) {
                say "    $file";
            }
            say '';
        }
    }
}
sub wipe_data {
    my $prefix = sprintf '%s:', NAMESPACE; 
    my $length = length $prefix;

    foreach my $key ( $redis->keys('*') ) {
        $redis->del( $key )
            if substr( $key, 0, $length ) eq $prefix;
    }
}

sub get_matching_files {
    my $file   = shift;
    my $digest = shift // get_sha_of_file( $file );

    my $key = sprintf '%s:%s', NAMESPACE, $digest;
    return $redis->hkeys( $key );
}
sub get_sha_of_file {
    my $file = shift;

    my $handle = FileHandle->new( $file );
    return unless defined $handle;

    my $sha = Digest::SHA1->new();
    $sha->addfile( $handle );

    return $sha->hexdigest;
}

sub register_file {
    my $file   = shift;
    my $digest = shift;

    my $stat = stat $file;

    my $key = sprintf '%s:%s', NAMESPACE, $digest;
    $redis->hset( $key, $file, $stat->size );

    my $count = $redis->hlen( $key );
    $redis->zadd( SET_KEY, $count, $key );
}
sub deregister_file {
    my $file   = shift;
    my $digest = shift;

    my $key = sprintf '%s:%s', NAMESPACE, $digest;
    $redis->hdel( $key, $file );

    my $count = $redis->hlen( $key );
    $redis->zadd( SET_KEY, $count, $key );
}

__END__

=head1 SYNOPSIS

B<duplicates> - find and remove duplicate files

=over

=item duplicates stats [-v]

Print some information about the duplicate files. Use C<-v> to list the top 
10 duplicates (in terms of file size).

=back
