#!/usr/bin/env perl

use Modern::Perl    '2012';
use Cwd             qw( abs_path );
use Digest::SHA1;
use File::Next;
use File::stat;
use FileHandle;
use Getopt::Std;
use Number::Bytes::Human    qw( format_bytes );
use Pod::Usage;
use Redis;
use Try::Tiny;

use constant NAMESPACE  => 'duplicates';
use constant SET_KEY    => sprintf '%s:%s', NAMESPACE, 'all';
use constant TOP_WASTED => 10;

$| = 1;
my $redis = Redis->new();

my $arg = shift;

my %option;
getopts('hv', \%option);
pod2usage() if $option{'h'} or !defined $arg or $arg eq '-h';


if ( 'stats' eq $arg ) {
    output_stats();
}
elsif ( 'scan' eq $arg ) {
    scan_directory( @ARGV );
}
elsif ( 'list' eq $arg ) {
    if ( $option{'a'} ) {
        list_all_duplicates()
    }
    else {
        list_duplicates_in_directory( @ARGV )
    }
}
elsif ( 'matches' eq $arg ) {
    die "Usage: duplicates matches <file> [...]\n"
        if scalar @ARGV == 0;
    list_matches( @ARGV );
}
elsif ( 'delete' eq $arg ) {
    die "Won't automatically delete files; at least one directory is required\n"
        if scalar @ARGV == 0;
    delete_duplicates( @ARGV );
}
elsif ( 'wipe' eq $arg ) {
    wipe_data();
}
elsif ( 'rescan' eq $arg ) {
    @ARGV = ( '/' )
        if $option{'a'};

    rescan_duplicates( @ARGV );
}
else {
    die "Unknown arg: $arg\n";
}
exit;



sub scan_directory {
    my @directories = @_;

    # default to current directory
    push @directories, '.'
        if $#directories < 0;

    foreach my $directory ( @directories ) {
        $directory = abs_path( $directory );
        say $directory
            if $option{'v'};

        my $count = 0;
        my $files = File::Next::files( $directory );
        while( my $file = $files->() ) {
            $count++;

            print status_line( "  [$count] $file" )
                if $option{'v'};

            try {
                my $digest = get_sha_of_file( $file );
                register_file( $file, $digest );
            }
            catch {
                warn "$file: $!";
            };

        }
        print status_line()
            if $option{'v'};
    }
}
sub list_duplicates_in_directory {
    my @directories = @_;

    # default to current directory
    push @directories, '.'
        if $#directories < 0;

    foreach my $directory ( @directories ) {
        $directory = abs_path( $directory );

        my @duplicates = $redis->zrevrangebyscore( SET_KEY, '+inf', 2 );
        my $count = 0;

        foreach my $dupe ( @duplicates ) {
            $dupe =~ m{:(.*)};

            my @keys = $redis->hkeys( $dupe );
            foreach my $key ( @keys ) {
                next unless $key =~ m{^$directory};
                say $key;
            }
        }
    }
}
sub list_all_duplicates {
    my @duplicates = $redis->zrevrangebyscore( SET_KEY, '+inf', 2 );
    foreach my $dupe ( @duplicates ) {
        $dupe =~ m{:(.*)};

        say "${1}:"
            if $option{'v'};
        foreach my $key ( $redis->hkeys( $dupe ) ) {
            say( ( $option{'v'} ? '    ' : '' ) . $key );
        }
        say ''
            if $option{'v'};
    }
}
sub list_matches {
    my @list = @_;

    foreach my $item ( @list ) {
        $item = abs_path( $item );

        say "${item}:"
            if $option{'v'};

        my @matches = get_matching_files( $item );
        foreach my $match ( @matches ) {
            next unless $match ne $item;
            say( ( $option{'v'} ? '    ' : '' ) . $match );
        }

        say ''
            if $option{'v'};
    }
}
sub delete_duplicates {
    my @list = @_;

    my $rescan_warning = 0;

    foreach my $directory ( @list ) {
        $directory = abs_path( $directory );

        my $files = File::Next::files( $directory );

        while( my $file = $files->() ) {
            print status_line( "  $file" )
                if $option{'v'};

            my $digest  = get_sha_of_file( $file );
            my @matches = get_matching_files( $file, $digest );
            
            # only delete it if other duplicates exist
            next unless $#matches;

            # double check those duplicates still exist
            foreach my $match ( @matches ) {
                next if $match eq $file;
                
                my $match_digest = get_sha_of_file( $match );
                if ( !defined $match_digest or $match_digest ne $digest ) {
                    $rescan_warning = $match;
                    next;
                }

                unlink $file
                    or die "Couldn't delete $file: $!";

                deregister_file( $file, $digest );

                print status_line( '' )
                    if $option{'v'};
                say "Deleted $file";
                last;
            }
        }
        chdir $directory;
        system 'find', '.', '-type', 'd', '-delete', '-empty';
    }

    print status_line()
        if $option{'v'};

    if ( $rescan_warning ) {
        warn "$rescan_warning no longer a duplicate\n";
        warn "Run 'duplicates rescan' and try again\n";
    }
}
sub rescan_duplicates {
    my @directories = @_;

    # default to current directory
    push @directories, '.'
        if $#directories < 0;

    # canonicalise once
    my @dirs;
    foreach my $directory ( @directories ) {
        push @dirs, abs_path( $directory );
    }

    my @duplicates = $redis->zrevrangebyscore( SET_KEY, '+inf', 1 );
    my $total = scalar @duplicates;
    my $count = 0;

    foreach my $duplicate ( @duplicates ) {
        $duplicate =~ m{:(.*)};
        my $orig_digest = $1;

        $count++;

        foreach my $file ( $redis->hkeys( $duplicate ) ) {
            my $within = 0;
            foreach my $directory ( @dirs ) {
                $within = 1
                    if $file =~ m{^$directory};
            }
            next unless $within;

            select undef, undef, undef, 0.01;

            print status_line( "  [$count/$total] $file" )
                if $option{'v'};

            my $digest = get_sha_of_file( $file ) // '';
            if ( $orig_digest ne $digest ) {
                print status_line( '' )
                    if $option{'v'};
                say "Updated $file";

                deregister_file( $file, $orig_digest );
                register_file( $file, $digest )
                    if $digest;
            }
        }
    }
    print status_line( '' )
        if $option{'v'};
}
sub output_stats {
    my @directories = $@;

    my @duplicates = $redis->zrevrangebyscore( SET_KEY, '+inf', 2 );

    my $count  = 0;
    my $wasted = 0;

    my $smallest = 0;
    my %top_files;

    foreach my $dupe ( @duplicates ) {
        my %keys = $redis->hgetall( $dupe );

        my( undef, $size ) = each %keys;
        my $dupes = scalar( keys %keys ) - 1;

        $wasted += ( $size * $dupes );
        $count += $dupes;

        if ( $option{'v'} ) {
            $top_files{$size} = []
                if ! defined $top_files{$size};
            push @{$top_files{$size}}, keys %keys;

            if ( $size > $smallest and scalar keys %top_files > TOP_WASTED ) {
                my @sizes = sort { $a <=> $b } keys %top_files;

                delete $top_files{$sizes[0]};
                my $smallest = $sizes[1];
            }
        }
    }

    say sprintf "%d duplicated files, %s wasted space",
        $count, format_bytes($wasted)
            if $count;

    if ( $option{'v'} ) {
        say '';
        for my $size ( sort { $b <=> $a } keys %top_files ) {
            say format_bytes($size);
            foreach my $file ( @{ $top_files{$size} } ) {
                say "    $file";
            }
            say '';
        }
    }
}
sub wipe_data {
    my $prefix = sprintf '%s:', NAMESPACE; 
    my $length = length $prefix;

    foreach my $key ( $redis->keys('*') ) {
        $redis->del( $key )
            if substr( $key, 0, $length ) eq $prefix;
    }
}

sub get_matching_files {
    my $file   = shift;
    my $digest = shift // get_sha_of_file( $file );

    my $key = sprintf '%s:%s', NAMESPACE, $digest;
    return $redis->hkeys( $key );
}
sub get_sha_of_file {
    my $file = shift;

    my $handle = FileHandle->new( $file );
    return unless defined $handle;

    my $sha = Digest::SHA1->new();
    $sha->addfile( $handle );

    return $sha->hexdigest;
}

sub register_file {
    my $file   = shift;
    my $digest = shift;

    my $stat = stat $file;

    my $key = sprintf '%s:%s', NAMESPACE, $digest;
    $redis->hset( $key, $file, $stat->size );

    my $count = $redis->hlen( $key );
    $redis->zadd( SET_KEY, $count, $key );
}
sub deregister_file {
    my $file   = shift;
    my $digest = shift;

    my $key = sprintf '%s:%s', NAMESPACE, $digest;
    $redis->hdel( $key, $file );

    my $count = $redis->hlen( $key );
    $redis->zadd( SET_KEY, $count, $key );
}
sub status_line {
    my $line = shift;

    # pad out, then trim
    $line .= ' ' x 79;
    $line  = substr $line, 0, 79;

    return "$line\r";
}

__END__

=head1 SYNOPSIS

B<duplicates> - find and remove duplicate files

=over

=item duplicates scan [-v] [<dir> ...]

Looks for duplicate files in <dir>s. Defaults to current directory.

=item duplicates list [<dir> ...]

Lists files in <dir>s that have duplicates elsewhere. Defaults to current
directory. 

=item duplicates list [-v] -a

List all registered matches. Use C<-v> to verbosely list the SHA1 and group
matches rather than just listing filenames.

=item duplicates matches [-v] <file> [...]

Lists the duplicates of <file>s. Use C<-v> to verbosely list the SHA1 and
group matches rather than just listing filenames.

=item duplicates delete [-v] <dir> [...]

Deletes any files within <dir>s that have duplicates elsewhere. Will re-read
the content of duplicate files before deleting to confirm they are still 
duplicates.

=item duplicates rescan [-v] [<dir> ...]

Re-reads all registered duplicate files (optionally only within <dir>s) to
confirm they are still duplicates. Defaults to re-reading all registered files.

=item duplicates stats [-v]

Print some information about the duplicate files. Use C<-v> to list the top 
10 duplicates (in terms of file size).

=item duplicates wipe

Removes all information stored in redis.

=back
